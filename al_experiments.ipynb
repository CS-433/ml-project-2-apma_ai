{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          Trainer)\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "class ActiveLearningModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Adapter for our LLM to be able to communicate with the teacher\n",
    "        dataset. It can contain any\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id, tok, teacher, full_model=None):\n",
    "        super().__init__()\n",
    "        if(full_model is None): ## if you want to input a model-id to load from HF\n",
    "          self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        else : ## if you want to input a full model with all its params\n",
    "          self.model = full_model\n",
    "        self.tok = tok\n",
    "        self.teacher = teacher ## teacher ds\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                labels=None) -> torch.Tensor:\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        ## send to teacher model\n",
    "        self.teacher.update(output.logits)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset class that process the samples and puts them on the necessary device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_length, device) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.initial_data = data ## used for introducing of randomness\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in your dataset\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve and preprocess a single sample at the given index\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Use the tokenizer to tokenize the input text\n",
    "        inputs = self.tokenizer(\n",
    "            sample[\"tweet\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # You might want to include other information such as labels\n",
    "\n",
    "        labels = sample.get(\"labels\", [])\n",
    "\n",
    "        # Return a dictionary containing the input_ids, attention_mask, and labels\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0).to(self.device),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).to(self.device),\n",
    "            \"labels\": torch.tensor(labels).to(self.device),\n",
    "        }\n",
    "\n",
    "class TeacherDataset(MyDataset):\n",
    "    \"\"\"\n",
    "        Teacher dataset can be updated using the given student model\n",
    "        updates\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length, device, T):\n",
    "        # self.data = self.load_data()\n",
    "        super().__init__(data, tokenizer, max_length, device)\n",
    "        self.T = T\n",
    "        self.allHs = torch.tensor([]).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, past_logits, automatic_new_iter=False):\n",
    "        Ps = torch.softmax(past_logits, dim=0)\n",
    "        nHs = (-Ps * torch.log(Ps)).sum(dim=-1).reshape((-1,))\n",
    "        if(len(self.allHs) + len(nHs) <= len(self.data)): ## test data\n",
    "          self.allHs = torch.cat([self.allHs, nHs])\n",
    "          if(automatic_new_iter and (self.allHs.size()[0] == len(self.data))):\n",
    "              self.new_iter()\n",
    "\n",
    "    def new_iter(self, add_randomness=False):\n",
    "        if(not add_randomness):\n",
    "          selected_idx = self.allHs.argsort()[-int(len(self.allHs) / 2):] ## take the sample above the entropy median\n",
    "          print(\"> TEACHER ROUND, from\", len(self.data), \"samples to\", len(selected_idx))\n",
    "          self.data = HFDataset.from_dict(self.data[selected_idx.tolist()])\n",
    "\n",
    "        else :\n",
    "          CHUNK_SIZE = int(1 * len(self.allHs) / 4)\n",
    "          high_H_idxs = self.allHs.argsort()[-CHUNK_SIZE:] ## 1/4 of high entropy samples\n",
    "          high_H_samples = HFDataset.from_dict(self.data[high_H_idxs.tolist()])\n",
    "          random_init_samples = HFDataset.from_dict(self.data[torch.randperm(len(self.data))[:CHUNK_SIZE]])\n",
    "          print(\"> TEACHER ROUND, from\", len(self.data), \"samples to\", len(high_H_idxs) + len(random_init_samples))\n",
    "          self.data = concatenate_datasets([random_init_samples, high_H_samples])\n",
    "\n",
    "        self.allHs = torch.tensor([]).to(self.device)\n",
    "\n",
    "        if(len(self.data) < self.T):\n",
    "          print(f\"Threshold was set at T = {self.T}, {len(self.data)} remaining datapoints, halting.\")\n",
    "          self.data = HFDataset.from_dict({}) ## empty dataset ===> halt\n",
    "\n",
    "\n",
    "class RunArgs():\n",
    "   \n",
    "   def __init__(self,\n",
    "                n_samples:int=200_000,\n",
    "                device:str=\"cuda:0\",\n",
    "                T:int=10_000,\n",
    "                test_ratio:float=0.1,\n",
    "                max_length:int=130,\n",
    "                batch_size:int=64,\n",
    "                learning_rate:float=2e-5,\n",
    "                weight_decay:float=0.01,\n",
    "                SAVE_DIR:str=\"models/\",\n",
    "                BASE_MODEL:str=\"vinai/bertweet-base\",\n",
    "                warmup_prcnt:float=.3) -> None:\n",
    "      self.n_samples = n_samples\n",
    "      self.device = device\n",
    "      self.T = T\n",
    "      self.test_ratio = test_ratio\n",
    "      self.max_length = max_length\n",
    "      self.batch_size = batch_size\n",
    "      self.learning_rate = learning_rate\n",
    "      self.weight_decay = weight_decay\n",
    "      self.SAVE_DIR = SAVE_DIR\n",
    "      self.BASE_MODEL = BASE_MODEL\n",
    "      self.warmup_prcnt = warmup_prcnt\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        labels = labels.long()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"])\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels.long())\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with randomness ::\n",
    "\n",
    "from utils import create_datasets\n",
    "from transformers import TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def run_with_args(args):\n",
    "        BERT_MODEL = args.BASE_MODEL\n",
    "        DIR = args.SAVE_DIR\n",
    "        tok = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "        tok.pad_token = tok.eos_token\n",
    "        ds = create_datasets(sub_sampling=args.n_samples) ## small dataset for testing\n",
    "        ds = ds.train_test_split(test_size=args.test_ratio)\n",
    "        ds_train, ds_test = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "        teacher = TeacherDataset(ds_train,\n",
    "                                tokenizer=tok,\n",
    "                                max_length=args.max_length,\n",
    "                                device=args.device,\n",
    "                                T=args.T)\n",
    "\n",
    "        test_data = MyDataset(ds_test, tokenizer=tok, max_length=args.max_length, device=args.device)\n",
    "\n",
    "        model = ActiveLearningModel(BERT_MODEL, tok=tok, teacher=teacher)\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            return accuracy.compute(predictions=predictions, references=labels)\n",
    "        total_steps = 2 * math.ceil(len(ds_train) / args.batch_size)\n",
    "        warmup_steps = int(args.warmup_prcnt * total_steps)\n",
    "        optimizer = AdamW(model.parameters(), lr=args.learning_rate, no_deprecation_warning=True)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        iter = 1\n",
    "        while(len(teacher) != 0):\n",
    "            ## train for 1 epoch = 1 round of the DataLoader\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=DIR,\n",
    "                per_device_train_batch_size=args.batch_size,\n",
    "                per_device_eval_batch_size=args.batch_size,\n",
    "                num_train_epochs=1, ## only 1 epoch\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                remove_unused_columns=False,\n",
    "            )\n",
    "\n",
    "            trainer = CustomTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=teacher,\n",
    "                eval_dataset=test_data,\n",
    "                tokenizer=tok,\n",
    "                optimizers=(optimizer, scheduler),\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            print(\"FINISHED EPOCH 1 ==> updating\")\n",
    "            teacher.new_iter(add_randomness=True) ## teacher round\n",
    "            model.model.save_pretrained(DIR+f\"epoch_{iter}/\")\n",
    "            iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = RunArgs(**{\n",
    "    \"n_samples\": 150_000,\n",
    "    \"device\": mps_device,\n",
    "    \"T\": 10_000,\n",
    "    \"max_length\": 130,\n",
    "    \"batch_size\":64\n",
    "}) ## default BERTweet Model\n",
    "run_with_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
